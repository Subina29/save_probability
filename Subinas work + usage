#import
import sklearn
import re
import scipy
import datetime
import dateutil.relativedelta
import os
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import pandas_gbq as pdq
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

# Cancel request dataset
project_id = 'infusionsoft-looker-poc'
sql = ("""
    SELECT *
    FROM `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_save_requests_table`
""")
dfTotal=pdq.read_gbq(sql, project_id=project_id, dialect='standard')
dfTotal = dfTotal[dfTotal['sales_cohort_date'].notna()]

# create a new column with refined app names
def refine_name(name):
    if(name.find('Cancelled')>0):
        if(name.find('-')>0):
            fname = name.split('-')[0]
        elif(name.find('–')>0):
            fname = name.split('–')[0]
    else:
        return name
    return(fname)
dfTotal['app_name_new'] = dfTotal['app_name'].apply(refine_name)
dfTotal = dfTotal.drop(['app_name'], axis=1)
dfTotal.reset_index(drop=True,inplace=True) ## index

# Usage dataset
project_id = 'infusionsoft-looker-poc'
sql = ("""
       select US.appname, format_date('%Y-%m',fdate) as YM, num_contacts, avg_emailsent, freq_emailsent, avg_contactupdate, freq_contactupdate, sum_goal_action, 
avg_emailsent/mean_emailsent as norm_emailsent, freq_emailsent/mean_freq_emailsent as norm_freq_emailsent, avg_contactupdate/mean_contactupdate as norm_contactupdate, freq_contactupdate/mean_freq_contactupdate as norm_freq_contactupdate, email_open, sum_goal_action/mean_sum as norm_sum_goal_action
from (
SELECT appname, min(date) as fdate, max(NUM_CONTACTS) as num_contacts,
ifnull(sum(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0)) / nullif(count(nullif(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0),0)),0),0) as avg_emailsent,
count(nullif(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0),0)) / count(appname) as freq_emailsent,
ifnull(sum(ifnull(CONTACTS_UPDATED,0)) / nullif(count(nullif(ifnull(CONTACTS_UPDATED,0),0)),0),0) as avg_contactupdate,
count(nullif(ifnull(CONTACTS_UPDATED,0),0)) / count(appname) as freq_contactupdate,
ifnull(sum(ifnull(NUMEMAILSOPENED_AUTO,0)+ifnull(NUMEMAILSOPENED_MANUAL,0)+ifnull(NUMEMAILSOPENED_BROADCAST,0)+ifnull(NUMEMAILSOPENED_AUTO_SYSTEM,0)) / nullif(sum(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0)),0),0) as email_open,
sum(ifnull(GOAL_ACHIEVED_COUNT,0)+ifnull(broadcasts_created,0)+ifnull(campaigns_created,0)+ifnull(funnel_created,0)+ifnull(funnel_published,0)+ifnull(actions_created,0)) as sum_goal_action
FROM `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_usage_data_table` as U
where (format_date('%Y-%m',date)!='2017-04')
and (timestamp(date) >= (select min(sales_cohort_date) from `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_save_requests_table` as S where U.appname=S.app_name group by app_name ))
group by appname, format_date('%Y-%m',date)
order by appname, fdate
) US
left join (
select appname, avg(nullif(avg_emailsent,0)) as mean_emailsent, avg(nullif(freq_emailsent,0)) as mean_freq_emailsent,avg(nullif(avg_contactupdate,0)) as mean_contactupdate, avg(nullif(freq_contactupdate,0)) as mean_freq_contactupdate, avg(nullif(email_open,0)) as mean_email_open, avg(nullif(sum_goal_action,0)) as mean_sum
from
(SELECT appname, min(date) as fdate,
ifnull(sum(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0)) / nullif(count(nullif(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0),0)),0),0) as avg_emailsent,
count(nullif(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0),0)) / count(appname) as freq_emailsent,
ifnull(sum(ifnull(CONTACTS_UPDATED,0)) / nullif(count(nullif(ifnull(CONTACTS_UPDATED,0),0)),0),0) as avg_contactupdate,
count(nullif(ifnull(CONTACTS_UPDATED,0),0)) / count(appname) as freq_contactupdate,
ifnull(sum(ifnull(NUMEMAILSOPENED_AUTO,0)+ifnull(NUMEMAILSOPENED_MANUAL,0)+ifnull(NUMEMAILSOPENED_BROADCAST,0)+ifnull(NUMEMAILSOPENED_AUTO_SYSTEM,0)) / nullif(sum(ifnull(NUMEMAILSSENT_AUTO,0)+ifnull(NUMEMAILSSENT_MANUAL,0)+ifnull(NUMEMAILSSENT_BROADCAST,0)+ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0)),0),0) as email_open,
sum(ifnull(GOAL_ACHIEVED_COUNT,0)+ifnull(broadcasts_created,0)+ifnull(campaigns_created,0)+ifnull(funnel_created,0)+ifnull(funnel_published,0)+ifnull(actions_created,0)) as sum_goal_action
FROM `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_usage_data_table` as U
where (format_date('%Y-%m',date)!='2017-04')
and (timestamp(date) >= (select min(sales_cohort_date) from `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_save_requests_table` as S where U.appname=S.app_name group by app_name ))
group by appname, format_date('%Y-%m',date)
order by appname, fdate)
group by appname
order by appname
) USM
on US.appname=USM.appname
order by US.appname, fdate
""")
nus=pdq.read_gbq(sql, project_id=project_id, dialect='standard')

nus['appname'] = nus['appname'].apply(refine_name)

sr2=dfTotal[['app_name_new','created_date']]
sr2['YM']=sr2['created_date'].dt.strftime('%Y-%m')

project_id = 'infusionsoft-looker-poc'
sql = ("""
    SELECT appname,
sum(ifnull(NUMEMAILSSENT_AUTO,0)) as AUTO, sum(ifnull(NUMEMAILSSENT_MANUAL,0)) as MANUAL,sum(ifnull(NUMEMAILSSENT_BROADCAST,0)) as BROADCAST, sum(ifnull(NUMEMAILSSENT_AUTO_SYSTEM,0)) as SYSTEM, sum(ifnull(CONTACTS_UPDATED,0)) as CONTACT_ONLY
FROM `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_usage_data_table` as U
where (format_date('%Y-%m',date)!='2017-04')
and (timestamp(date) >= (select min(sales_cohort_date) from `infusionsoft-looker-poc.asu_msba_save_probability.CONFIDENTIAL_save_requests_table` as S where U.appname=S.app_name group by app_name ))
group by appname
""")
usaut=pdq.read_gbq(sql, project_id=project_id, dialect='standard')
usaut.sort_values(by=['appname'],inplace=True)
usaut.reset_index(drop=True,inplace=True)
usaut['appname'] = usaut['appname'].apply(refine_name)
usaut=usaut.groupby('appname').agg('sum')
usaut['CONTACT_ONLY']=np.where(usaut['CONTACT_ONLY'].values >= 1, 1,0)
usaut['main_use']=usaut.idxmax(axis=1)
usaut=usaut.drop(['AUTO','MANUAL','BROADCAST','SYSTEM','CONTACT_ONLY'],axis=1)
nus=nus.fillna(0)
sr2=pd.merge(sr2,usaut,left_on='app_name_new',right_index=True,how='left')
names=list(nus)[8:14]
names3 = []
for i in range(3,0,-1):
    for j in range(len(names)):
        names3.append(str(i)+" "+names[j])
for i in names3:
    sr2[i]=np.nan
for i in range(len(sr2)):
    flmon=nus[nus['appname']==sr2['app_name_new'][i]]
    a1=flmon[flmon['YM']<=sr2['YM'][i]].sort_values(by=['YM']).iloc[-4:,8:14]
    sr2.iloc[i,4:10]=list(a1.iloc[-4:,].sum()-3*a1.iloc[-3:,].mean())
    sr2.iloc[i,10:16]=list(a1.iloc[-3:,].sum()-2*a1.iloc[-2:,].mean())
    sr2.iloc[i,16:22]=list(a1.iloc[-2:,].sum()-a1.iloc[-1:,].mean())
sr2=sr2.iloc[:,3:]
dm=pd.get_dummies(sr2.iloc[:,0])
sr2=pd.concat([sr2,dm],sort=False,axis=1)
sr2.drop('main_use',axis=1,inplace=True)
sr2=sr2.fillna(0)
# Merging Datasets
dfTotal=pd.concat([dfTotal,sr2],sort=False,axis=1)
dfTotal.to_csv("dfTotal.csv")   #### DELETE THIS

#Make Target
def create_target(ser):
    ser['lost_rev_diff'] = (pd.to_datetime(ser['lost_revenue_date']) - pd.to_datetime(ser['closed_date']))/np.timedelta64(1, 'D')
    ser['my_target'] = np.where(((ser['lost_revenue_date'].isna()) | (ser['lost_rev_diff']>60)), 0, 1)
    ser['my_target'] = np.where(((ser['lost_revenue_date'].notna()) & (ser['lost_rev_diff']<=60)), 1, 0)
    return ser

dfTotal = dfTotal.groupby('app_name_new').apply(create_target) 

# Drop the second lost - one app can be lost just once
def checkMultiLost(group):
    group1 = group[group['my_target']==1]
    group1 = group1.sort_values('created_date',ascending=False)
    group1 = group1.reset_index()
    group1 = group1.drop(group1.index[1:])
    group = group[~group['my_target']==1]
    group = group.append(group1, sort=True)
    return group
    
def check(ser):
    a = ser.filter(lambda x:x['my_target'].sum()>=2)
    a = a.groupby("app_name_new").apply(checkMultiLost)
    return a

grpbd = dfTotal.groupby("app_name_new")
df_new = grpbd.pipe(check)
df_new.shape

dfTotal = dfTotal[~dfTotal.app_name_new.isin(df_new['app_name_new'])]
dfTotal.shape
dfTotal = dfTotal.append(df_new, sort=True)
dfTotal.shape
dfTotal.reset_index(drop=True,inplace=True) ## index

# IMPUTATION
# years_in_business
dfTotal['years_in_business'].value_counts()
# For bin-checking of yrs_in_business
df_checkbin, df_rest = train_test_split(dfTotal, test_size=0.95)
df_checkbin.shape
total_saved = df_checkbin[df_checkbin['years_in_business'].notna() & df_checkbin['target_saved']==1].shape[0]
df_checkbin.groupby('years_in_business')['target_saved'].apply(lambda x:x.sum()/total_saved)
# For bin-checking of industry
total_tar_saved = df_checkbin[df_checkbin['industry'].notna() & df_checkbin['target_saved']==1].shape[0]
df_checkbin.groupby("industry")['target_saved'].apply(lambda x:x.sum()/total_tar_saved) 
# region_2
si_region_2 = SimpleImputer(missing_values=None, strategy='constant', fill_value='Unknown')
df_rest['region_2_imp'] = pd.DataFrame(si_region_2.fit_transform(df_rest[['region_2']]), index=df_rest.index)
df_rest = df_rest.drop('region_2', axis=1)
df_rest['region_2_imp'].value_counts()
oe_region_2 = OrdinalEncoder()
df_rest['region_2_enc'] = oe_region_2.fit_transform(df_rest[['region_2_imp']])
df_rest = df_rest.drop('region_2_imp', axis=1)
df_rest['region_2_enc'].value_counts()
# number_of_employees
df_rest['number_of_employees'].value_counts()
# number_of_employees
si_region_2 = SimpleImputer(missing_values=None, strategy='constant', fill_value='Unknown')
df_rest['number_of_employees_imp'] = pd.DataFrame(si_region_2.fit_transform(df_rest[['number_of_employees']]), index=df_rest.index)
df_rest['number_of_employees_imp'].value_counts()
oe_num_of_emp = OrdinalEncoder()
df_rest['number_of_employees_enc'] = oe_num_of_emp.fit_transform(df_rest[['number_of_employees_imp']])

df_new_subset = df_rest[df_rest['years_in_business'].notna()].copy()
df_new_subset.shape
df_new_subset_rest = df_rest[df_rest['years_in_business'].isna()].copy()
df_new_subset_rest.shape
def create_new_yrs_in_bus(val):
    if((val=="1 to 2 Years") | (val=="3 to 4 Years")):
        val = "1 to 4 Years"
    else:
        return val
    return val
df_new_subset['years_in_business_new'] = df_new_subset['years_in_business'].transform(create_new_yrs_in_bus)
df_new_subset['years_in_business_new'].value_counts()
df_new_subset['years_in_business_flag'] = np.where(df_new_subset['years_in_business']==df_new_subset['years_in_business_new'], 0, 1)
df_new_subset['years_in_business_imp_flag'] = 0
df_new_subset['years_in_business_new'].unique()
# years_in_business
oe_yrs_in_bus = OrdinalEncoder()
df_new_subset['years_in_business_enc'] = oe_yrs_in_bus.fit_transform(df_new_subset[['years_in_business_new']])
df_new_subset = df_new_subset.drop(['years_in_business'], axis=1)
df_new_subset.rename(columns={'years_in_business_new': 'years_in_business'}, inplace=True)

# For KNN of yrs_in_business
df, dftest = train_test_split(df_new_subset, test_size=0.1)
X_train = df[['number_of_employees_enc', 'region_2_enc']]
y_train = df['years_in_business_enc']
X_valid = dftest[['number_of_employees_enc', 'region_2_enc']]
y_valid = dftest['years_in_business_enc']

from sklearn.neighbors import KNeighborsClassifier
knn_yrs_in_bus = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='auto', leaf_size=30, p=2, 
                           metric='minkowski')
knn_yrs_in_bus.fit(X_train, y_train)
y_pred = knn_yrs_in_bus.predict(X_valid)
from sklearn import metrics
print (metrics.accuracy_score(y_valid, y_pred))

# elbow-method
errorlst = pd.DataFrame(data=None, columns=['k','error'])
cv_scores = []
myList = list(range(1,10))
# subsetting just the odd ones
neighbors = filter(lambda x: x % 2 != 0, myList)
from sklearn.model_selection import cross_val_score
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_valid)
    error = metrics.mean_absolute_error (y_valid, y_pred)
    errorlst = errorlst.append ({'k':k, 'error':error}, ignore_index=True)
plt.plot (errorlst['k'], errorlst['error'], 'o-')

# impute missing values in years_in_business
X_test = df_new_subset_rest[['number_of_employees_enc', 'region_2_enc']]
df_new_subset_rest['years_in_business_enc'] = knn_yrs_in_bus.predict(X_test)
df_new_subset_rest['years_in_business_flag'] = np.where(df_new_subset_rest['years_in_business_enc']==0, 1, 0)
df_new_subset_rest['years_in_business_imp_flag'] = 1
df_new_subset_rest['years_in_business'] = oe_yrs_in_bus.inverse_transform(df_new_subset_rest[['years_in_business_enc']])
df_rest = df_new_subset.append(df_new_subset_rest, ignore_index=True, verify_integrity=True, sort=True)

def getBound(val, interest):
    if(interest=="LB"):
        if(val=="Starting in the next 60 days"):
            return -1
        elif(val=="No Answer"):
            return 0
        elif(val=="Less Than a Year"):
            return 0.1
        elif(val=="1 to 4 Years"):
            return 1
        else:
            return 5
    elif(interest=="UB"):
        if(val=="Starting in the next 60 days"):
            return -1
        elif(val=="No Answer"):
            return 0
        elif(val=="Less Than a Year"):
            return 0.9
        elif(val=="1 to 4 Years"):
            return 4
        else:
            return 6
    else:
        if(val=="Starting in the next 60 days"):
            return -1
        elif(val=="No Answer"):
            return 0
        elif(val=="Less Than a Year"):
            return 0.5
        elif(val=="1 to 4 Years"):
            return 2.5
        else:
            return 5.5 

def getNumOfEmps(val):
    if(val=="4 to 10"):
        return 7
    elif(val=="2 to 3"):
        return 2.5
    elif(val=="1"):
        return 0.5
    elif(val=="11 to 25"):
        return 18
    elif(val=="26 to 99"):
        return 62.5
    elif(val=="100 or more"):
        return 150
    else:
        return 0

df_rest['years_in_business_lb'] = df_rest['years_in_business'].apply(getBound, interest="LB")
df_rest['years_in_business_ub'] = df_rest['years_in_business'].apply(getBound, interest="UB")
df_rest['years_in_business_mean'] = df_rest['years_in_business'].apply(getBound, interest="mean")

# number_of_employees
df_rest['number_of_employees_mean'] = df_rest['number_of_employees'].apply(getNumOfEmps)

df_rest['created_date_only'] = [d.date() for d in df_rest['created_date']]

# years_in_business_mean
def getMax(ser):
    ser = ser.sort_values(by='years_in_business_mean', ascending=False)
    ser = ser.reset_index()
    ser = ser.drop(ser.index[1:])
    return(ser)
def entry_aggregate(grp):
    a = grp.filter(lambda x:x.shape[0]>=2)
    a = a.groupby(['app_name_new', 'created_date_only']).apply(getMax)
    return(a)

grp_entries = df_rest.groupby(['app_name_new', 'created_date_only'])
df_mult_entry = grp_entries.pipe(entry_aggregate)

df_mult_entry = df_mult_entry.reset_index(drop = True)
df_rest = df_rest.loc[~(df_rest.app_name_new.isin(df_mult_entry.app_name_new) & df_rest.created_date_only.isin(df_mult_entry.created_date_only)), :]
df_rest[df_rest['app_name_new'] == 'ac437'][['app_name_new', 'created_date', 'created_date_only']]
df_rest = df_rest.append(df_mult_entry, sort=True)

# Flatten row
def flatten_row(ser):
    ser['save_count'] = ser['target_saved'].sum()
    if(ser.shape[0]>=2):
        ser = ser.sort_values(by="created_date", ascending=False)
        ser = ser.reset_index(drop=True)
        indmax = ser['years_in_business_mean'].idxmax()
        indempmax = ser['number_of_employees_mean'].idxmax()
        ser.iloc[0]['years_in_business'] = ser.iloc[indmax]['years_in_business']
        ser.iloc[0]['years_in_business_mean'] = ser.iloc[indmax]['years_in_business_mean']
        ser.iloc[0]['years_in_business_enc'] = ser.iloc[indmax]['years_in_business_enc']
        ser.iloc[0]['years_in_business_flag'] = ser.iloc[indmax]['years_in_business_flag']
        ser.iloc[0]['years_in_business_imp_flag'] = ser.iloc[indmax]['years_in_business_imp_flag']
        
        ser.iloc[0]['number_of_employees'] = ser.iloc[indempmax]['number_of_employees']
        ser.iloc[0]['number_of_employees_enc'] = ser.iloc[indempmax]['number_of_employees_enc']
        ser.iloc[0]['number_of_employees_imp'] = ser.iloc[indempmax]['number_of_employees_imp']
        ser.iloc[0]['number_of_employees_mean'] = ser.iloc[indempmax]['number_of_employees_mean']
        
        ser = ser.drop(ser.index[1:])
    return(ser)
        
df_rest1 = df_rest.groupby('app_name_new').apply(flatten_row)
df_rest = df_rest.loc[~(df_rest.app_name_new.isin(df_rest1.app_name_new))]
df_rest = df_rest.append(df_rest1, sort=True)
df_rest = df_rest.reset_index(drop=True)

#TRAIN TEST SPLIT
X = df_rest.drop(['calls_attempted', 'calls_connected', 'closed_date', 'closed_lost_rev_diff', 'created_date', 'created_date_only', 'index','level_0',
                  'lost_rev_diff', 'lost_revenue_date', 'my_target', 'number_of_employees', 'number_of_employees_mean', 'number_of_employees_imp', 'region', 'sales_cohort_date', 'years_in_business', 
                  'years_in_business_lb', 'years_in_business_mean', 'years_in_business_ub', 'case_owner', 'cancel_request_type'], axis=1)
y = df_rest['my_target']

from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)
Xtrain = Xtrain.copy()
ytrain = ytrain.copy()
Xtest = Xtest.copy()
ytest = ytest.copy()

Xtrain.isna().sum()

# billing_frequency
# oe_billing_fre = OrdinalEncoder()
# Xtrain['billing_frequency_enc'] = oe_billing_fre.fit_transform(Xtrain[['billing_frequency']])
oe_billing_fre=OneHotEncoder(sparse=False, dtype='int', handle_unknown='ignore')
billing_freq=pd.DataFrame(oe_billing_fre.fit_transform(Xtrain[['billing_frequency']]),index=Xtrain.index, columns=oe_billing_fre.get_feature_names())
billing_freq = billing_freq.add_prefix('billing_frequency_')
Xtrain=pd.concat([Xtrain,billing_freq],axis=1)
Xtrain = Xtrain.drop(['billing_frequency'], axis=1)

# cancel_request_reason
Xtrain['cancel_request_reason'].value_counts()
si_reason = SimpleImputer(missing_values=None, strategy="constant", fill_value="Missing")
Xtrain["cancel_request_reason_si_raw"] = pd.DataFrame(si_reason.fit_transform(Xtrain[["cancel_request_reason"]]),
  index=Xtrain.index)
Xtrain["cancel_request_reason_si_raw"].isna().sum()
Xtrain["cancel_request_reason_si_raw"].unique()

# 2) Encoder categorical
crr_oe = OrdinalEncoder(categories="auto", dtype=int)
Xtrain["cancel_request_reason_oe"] = pd.DataFrame(crr_oe.fit_transform(Xtrain[["cancel_request_reason_si_raw"]]),
  index=Xtrain.index)
Xtrain.groupby(["cancel_request_reason_oe"]).count()["app_name_new"]

# 3) def kbins
def cancelReasonBins(reason):
    if reason in [1,4,6,7,12,16]:
        return 0
    elif reason in [0,3,5,9,10,11,15]:
        return 1
    elif reason in [2,8]:
        return 2
    elif reason == 14:
        return 3
    elif reason == 13:
        return 4

Xtrain["cancel_request_reason_bins"] = Xtrain["cancel_request_reason_oe"].apply(cancelReasonBins)
Xtrain.drop(['cancel_request_reason_si_raw', 'cancel_request_reason', 'cancel_request_reason_oe'], axis=1, inplace=True)

def getedi(x):
    if pd.notna(x):
        if x in ['New 2019','New UI']:
            return 'KEAP'
        else:
            return 'Infusion'
    else:
        return 'Infusion'
Xtrain['bin_edition_category']=Xtrain['edition_category'].apply(getedi)

ed_ohe=OneHotEncoder(sparse=False, dtype='int', handle_unknown='ignore')
ed=pd.DataFrame(ed_ohe.fit_transform(Xtrain[['bin_edition_category']]),index=Xtrain.index, columns=ed_ohe.get_feature_names())
ed = ed.add_prefix('edit_')
Xtrain=pd.concat([Xtrain,ed],axis=1)
Xtrain.drop(['edition_category'],axis=1,inplace=True)
Xtrain.head()

Xtrain['entered_on_promo_boolean_enc'] = Xtrain['entered_on_promo_boolean'].apply(lambda x:1 if x==True else 0)

Xtrain.drop('entered_on_promo_boolean',axis=1, inplace=True)

si_gif = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)
Xtrain['refinery_gi_flag_imp'] = pd.DataFrame(si_gif.fit_transform(Xtrain[['refinery_generic_insurance_flag']]), index=Xtrain.index)

ohe_gif = OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')
Xcat_gif = pd.DataFrame(ohe_gif.fit_transform(Xtrain[['refinery_gi_flag_imp']]), index=Xtrain.index, columns=ohe_gif.get_feature_names())
Xcat_gif = Xcat_gif.add_prefix('refinery_gi_')
Xtrain = pd.concat([Xtrain, Xcat_gif], axis=1)
Xtrain = Xtrain.drop(['refinery_generic_insurance_flag', 'refinery_gi_flag_imp'], axis=1)
Xtrain.head()

# refinery_generic_real_estate_flag
si_rgref = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)
Xtrain['refinery_gre_flag_imp'] = pd.DataFrame(si_rgref.fit_transform(Xtrain[['refinery_generic_real_estate_flag']]), index=Xtrain.index)
# refinery_generic_real_estate_flag
ohe_rgref = OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')
Xcat_rgref = pd.DataFrame(ohe_rgref.fit_transform(Xtrain[['refinery_gre_flag_imp']]), index=Xtrain.index, columns=ohe_rgref.get_feature_names())
Xcat_rgref = Xcat_rgref.add_prefix('refinery_gre_')
Xtrain = pd.concat([Xtrain, Xcat_rgref], axis=1)
Xtrain = Xtrain.drop(['refinery_generic_real_estate_flag', 'refinery_gre_flag_imp'], axis=1)
Xtrain.head()

# refinery_real_estate_flag
si_ref = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)
Xtrain['refinery_re_flag_imp'] = pd.DataFrame(si_ref.fit_transform(Xtrain[['refinery_real_estate_flag']]), index=Xtrain.index)
ohe_ref = OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')
Xcat_ref = pd.DataFrame(ohe_ref.fit_transform(Xtrain[['refinery_re_flag_imp']]), index=Xtrain.index, columns=ohe_ref.get_feature_names())
Xcat_ref = Xcat_ref.add_prefix('refinery_re_')
Xtrain = pd.concat([Xtrain, Xcat_ref], axis=1)
Xtrain = Xtrain.drop(['refinery_real_estate_flag', 'refinery_re_flag_imp'], axis=1)
Xtrain.head()

# sales_channel
si_sale = SimpleImputer(missing_values=None, strategy="constant", fill_value="Missing")
Xtrain["sales_channel_si_raw"] = pd.DataFrame(si_sale.fit_transform(Xtrain[["sales_channel"]]), index=Xtrain.index)
oe_sales_channel = OrdinalEncoder()
Xtrain['sales_channel_si_oe'] = pd.DataFrame(oe_sales_channel.fit_transform(Xtrain[['sales_channel_si_raw']]), index=Xtrain.index)

Xtrain['sales_channel_flag']=np.where(Xtrain["sales_channel"]!= Xtrain["sales_channel_si_raw"],1,0)
Xtrain = Xtrain.drop(['sales_channel', 'sales_channel_si_raw'], axis=1)
# Xtrain = pd.concat([Xtrain, Xcat_cancel], axis=1)
Xtrain.head()

# industry
si_industry = SimpleImputer(missing_values=None, strategy="constant", fill_value="Missing")
Xtrain["industry_si_raw"] = pd.DataFrame(si_industry.fit_transform(Xtrain[["industry"]]), index=Xtrain.index)

def industry_bins(val):
    if(val in ['Automotive', 'Chemicals', 'Electrical Equipment', 'Family Services', 'Hobbies, Sports & Leisure',
              'Insurance', 'Leisure Products', 'Medical or Dental (Practice, Devices, or Care)']):
        return 1
    elif(val in ['Air Freight & Logistics', 'Capital Markets', 'Communications Equipment',
                'Consumer Goods', 'Diversified Financial Services', 'Food Products',
                'Home Services', 'Hotels, Restaurants & Leisure', 'Machinery',
                'Manufacturing', 'Other']):
        return 2
    elif(val in ['Arts, Entertainment and Music', 'Beauty & Cosmetics Goods', 'Diversified Telecommunication Services',
                'Financial Services and Insurance (financial planning, wealth mgmt.)', 'Household Durables',
                'Non-profit', 'Personal Services (Salons, vets, etc.)', 'Pharmaceuticals',
                'Business Consulting & Coaching', 'Construction & Engineering', 'Consumer Staples', 
                 'Diversified Consumer Services', 'Media', 'News, Bloggers, & Online Communities',
                'Healthcare and Devices', 'Fitness and Wellness', 'Business Services (Accounting, legal, consulting, etc.)']):
        return 3
    elif(val in ['Consumer Discretionary', 'Education, Coaching and Speakers', 'Marketing, Design, or Advertising Services']):
        return 4
    elif(val in 'Missing'):
        return 5
    else:
        return 6
        
Xtrain['industry_bins'] = Xtrain['industry_si_raw'].transform(industry_bins)
Xtrain = Xtrain.drop(['industry', 'industry_si_raw'], axis=1)
    
# days_on_book
from sklearn.preprocessing import QuantileTransformer
days_on_book_scaler = QuantileTransformer(n_quantiles=10, output_distribution='uniform')
Xtrain['days_on_book_scaled'] = pd.DataFrame(days_on_book_scaler.fit_transform(Xtrain[['days_on_book']]), index=Xtrain.index)

# Impute negative mrr_amount with nan
Xtrain.loc[Xtrain['mrr_amount']<0, 'mrr_amount'] = np.nan
def imputeEditionMean(df):
    return df.fillna(df.mean())
edition_cat_group = Xtrain.groupby(['bin_edition_category'])
Xtrain['mrr_amount_imp'] = edition_cat_group[['mrr_amount']].transform(imputeEditionMean)
Xtrain['mrr_amount_imp_flag'] = np.where(Xtrain['mrr_amount'] != Xtrain['mrr_amount_imp'], 1, 0)

Xtrain = Xtrain.drop(['app_name_new', 'target_saved', 'bin_edition_category', 'mrr_amount'], axis=1)
Xtrain.info()

# random forest classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=10, 
                             max_features='auto', bootstrap=True,
                             max_depth=None, min_samples_split=2, min_samples_leaf=1, class_weight=None)
rfc.fit(Xtrain, ytrain)

# test set transformations
# billing_frequency
billing_freq=pd.DataFrame(oe_billing_fre.transform(Xtest[['billing_frequency']]),index=Xtest.index, columns=oe_billing_fre.get_feature_names())
billing_freq = billing_freq.add_prefix('billing_frequency_')
Xtest=pd.concat([Xtest,billing_freq],axis=1)
Xtest = Xtest.drop(['billing_frequency'], axis=1)
# cancel_request_reason
Xtest["cancel_request_reason_si_raw"] = pd.DataFrame(si_reason.transform(Xtest[["cancel_request_reason"]]),
  index=Xtest.index)
Xtest["cancel_request_reason_oe"] = pd.DataFrame(crr_oe.transform(Xtest[["cancel_request_reason_si_raw"]]),
  index=Xtest.index)
Xtest["cancel_request_reason_bins"] = Xtest["cancel_request_reason_oe"].apply(cancelReasonBins)
Xtest.drop(['cancel_request_reason_si_raw', 'cancel_request_reason', 'cancel_request_reason_oe'], axis=1, inplace=True)
# edition_category
Xtest['bin_edition_category']=Xtest['edition_category'].apply(getedi)
test_ed=pd.DataFrame(ed_ohe.transform(Xtest[['bin_edition_category']]),index=Xtest.index)
test_ed = test_ed.add_prefix('edit_')
Xtest=pd.concat([Xtest,test_ed],axis=1)
Xtest.drop(['edition_category'],axis=1,inplace=True)
# entered_on_promo_boolean
Xtest['entered_on_promo_boolean_enc'] = Xtest['entered_on_promo_boolean'].apply(lambda x:1 if x==True else 0)
Xtest.drop('entered_on_promo_boolean',axis=1, inplace=True)
# refinery_generic_insurance_flag
Xtest['refinery_gi_flag_imp'] = pd.DataFrame(si_gif.transform(Xtest[['refinery_generic_insurance_flag']]), index=Xtest.index)
# refinery_generic_insurance_flag
Xcat_gif_test = pd.DataFrame(ohe_gif.transform(Xtest[['refinery_gi_flag_imp']]), index=Xtest.index, columns=ohe_gif.get_feature_names())
Xcat_gif_test = Xcat_gif_test.add_prefix('refinery_gi_')
Xtest = pd.concat([Xtest, Xcat_gif_test], axis=1)
Xtest = Xtest.drop(['refinery_generic_insurance_flag', 'refinery_gi_flag_imp'], axis=1)
# refinery_generic_real_estate_flag
Xtest['refinery_gre_flag_imp'] = pd.DataFrame(si_rgref.transform(Xtest[['refinery_generic_real_estate_flag']]), index=Xtest.index)
Xcat_rgref_test = pd.DataFrame(ohe_rgref.transform(Xtest[['refinery_gre_flag_imp']]), 
                               index=Xtest.index, columns=ohe_rgref.get_feature_names())
Xcat_rgref_test = Xcat_rgref_test.add_prefix('refinery_gre_')
Xtest = pd.concat([Xtest, Xcat_rgref_test], axis=1)
Xtest = Xtest.drop(['refinery_generic_real_estate_flag', 'refinery_gre_flag_imp'], axis=1)
# refinery_real_estate_flag
Xtest['refinery_re_flag_imp'] = pd.DataFrame(si_ref.transform(Xtest[['refinery_real_estate_flag']]), index=Xtest.index)
Xcat_ref_test = pd.DataFrame(ohe_ref.transform(Xtest[['refinery_re_flag_imp']]), index=Xtest.index, columns=ohe_ref.get_feature_names())
Xcat_ref_test = Xcat_ref_test.add_prefix('refinery_re_')
Xtest = pd.concat([Xtest, Xcat_ref_test], axis=1)
Xtest = Xtest.drop(['refinery_real_estate_flag', 'refinery_re_flag_imp'], axis=1)
# sales_channel
Xtest["sales_channel_si_raw"] = pd.DataFrame(si_sale.transform(Xtest[["sales_channel"]]), index=Xtest.index)
Xtest['sales_channel_si_oe'] = pd.DataFrame(oe_sales_channel.transform(Xtest[['sales_channel_si_raw']]), 
                                             index=Xtest.index)
Xtest['sales_channel_flag']=np.where(Xtest["sales_channel"]!= Xtest["sales_channel_si_raw"],1,0)
Xtest = Xtest.drop(['sales_channel', 'sales_channel_si_raw'], axis=1)
# industry
Xtest["industry_si_raw"] = pd.DataFrame(si_industry.transform(Xtest[["industry"]]), index=Xtest.index)
Xtest['industry_bins'] = Xtest['industry_si_raw'].transform(industry_bins)
#Xtest['industry_oe'] = pd.DataFrame(oe_industry.transform(Xtest[['industry_bins']]), index=Xtest.index)
Xtest = Xtest.drop(['industry', 'industry_si_raw'], axis=1) #, 'industry_bins'
# days_on_book
Xtest['days_on_book_scaled'] = pd.DataFrame(days_on_book_scaler.transform(Xtest[['days_on_book']]), index=Xtest.index)
# mrr_amount
Xtest.loc[Xtest['mrr_amount']<0, 'mrr_amount'] = np.nan
# edition_category
edition_cat_group_test = Xtest.groupby(['bin_edition_category'])
Xtest['mrr_amount_imp'] = edition_cat_group_test[['mrr_amount']].transform(imputeEditionMean)
Xtest['mrr_amount_imp_flag'] = np.where(Xtest['mrr_amount'] != Xtest['mrr_amount_imp'], 1, 0)
Xtest = Xtest.drop(['app_name_new', 'target_saved', 'bin_edition_category','mrr_amount'], axis=1)


## ALGORITHM
# RandomForest
ypred = rfc.predict(Xtest)
print (rfc.score(Xtest, ytest))
from sklearn import metrics
print (metrics.accuracy_score(ytest, ypred))
print (metrics.precision_score(ytest, ypred))
print (metrics.confusion_matrix(ytest, ypred))
print (metrics.classification_report(ytest, ypred))

# LogisticRegression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(penalty='l2', dual=False, C=1.0, class_weight=None, solver='lbfgs', max_iter=8000)
lr.fit(Xtrain, ytrain)
ypred1 = lr.predict(Xtest)
print (lr.score(Xtest, ytest))
print (metrics.accuracy_score(ytest, ypred1))
print (metrics.precision_score(ytest, ypred1))
print (metrics.confusion_matrix(ytest, ypred1))
print (metrics.classification_report(ytest, ypred1))

# GaussianNB
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(Xtrain, ytrain)
ypred2 = gnb.predict(Xtest)
print (gnb.score(Xtest, ytest))
print (metrics.accuracy_score(ytest, ypred2))
print (metrics.precision_score(ytest, ypred2))
print (metrics.confusion_matrix(ytest, ypred2))
print (metrics.classification_report(ytest, ypred2))

from sklearn import svm
svc = svm.SVC(gamma='scale')
svc.fit(Xtrain, ytrain)
ypred3 = svc.predict(Xtest)
print (svc.score(Xtest, ytest))
print (metrics.accuracy_score(ytest, ypred3))
print (metrics.precision_score(ytest, ypred3))
print (metrics.confusion_matrix(ytest, ypred3))
print (metrics.classification_report(ytest, ypred3))

from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)
bagging.fit(Xtrain, ytrain)
ypred4 = bagging.predict(Xtest)
print (bagging.score(Xtest, ytest))
print (metrics.accuracy_score(ytest, ypred4))
print (metrics.precision_score(ytest, ypred4))
print (metrics.confusion_matrix(ytest, ypred4))
print (metrics.classification_report(ytest, ypred4))

from sklearn.ensemble import AdaBoostClassifier
adbr = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R')
adbr.fit(Xtrain, ytrain)
ypred5 = adbr.predict(Xtest)
print (adbr.score(Xtest, ytest))
print (metrics.accuracy_score(ytest, ypred5))
print (metrics.precision_score(ytest, ypred5))
print (metrics.confusion_matrix(ytest, ypred5))
print (metrics.classification_report(ytest, ypred5))
